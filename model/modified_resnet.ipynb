{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Installing Required Libraries\n","This cell installs the necessary Python libraries for the project.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:12:17.874458Z","iopub.status.busy":"2025-03-15T04:12:17.874178Z","iopub.status.idle":"2025-03-15T04:12:34.282508Z","shell.execute_reply":"2025-03-15T04:12:34.281624Z","shell.execute_reply.started":"2025-03-15T04:12:17.874436Z"}},"outputs":[],"source":["!pip install torchsummary\n","!pip install torchinfo\n","!pip install torch_optimizer\n","!pip install torchvision\n"]},{"cell_type":"markdown","metadata":{},"source":["## Importing all the required libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:12:38.823874Z","iopub.status.busy":"2025-03-15T04:12:38.823551Z","iopub.status.idle":"2025-03-15T04:12:41.786373Z","shell.execute_reply":"2025-03-15T04:12:41.785516Z","shell.execute_reply.started":"2025-03-15T04:12:38.823847Z"},"trusted":true},"outputs":[],"source":["import torch\n","import pickle\n","import numpy as np\n","import pandas as pd \n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.cuda.amp as amp\n","from torchinfo import summary\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import torch_optimizer as optim2\n","from torchsummary import summary\n","from torch.utils.data import Dataset\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from torch.cuda.amp import autocast, GradScaler\n","from torchvision.transforms import AutoAugmentPolicy\n","from torch.optim.lr_scheduler import CosineAnnealingLR"]},{"cell_type":"markdown","metadata":{},"source":["## Data Transformations and Loaders\n","The following code configures data augmentation and normalization for training and testing datasets. It loads the CIFAR-10 training and test batches from pickle files, creates datasets using the CIFARDataset class, and sets up DataLoader objects for efficient batch processing."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:13:14.413314Z","iopub.status.busy":"2025-03-15T04:13:14.412831Z","iopub.status.idle":"2025-03-15T04:13:14.420321Z","shell.execute_reply":"2025-03-15T04:13:14.419439Z","shell.execute_reply.started":"2025-03-15T04:13:14.413291Z"},"id":"y8ixXYZ_zSnd","trusted":true},"outputs":[],"source":["class CIFARDataset(Dataset):\n","    def __init__(self, data, labels=None, transform=None):\n","\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x = self.data[idx]\n","\n","        # 1) If shape is (3072,) => reshape to (3,32,32)\n","        if x.shape[0] == 3072 and x.ndim == 1:\n","            x = x.reshape(3, 32, 32)\n","\n","        # 2) If shape is (32,32,3) => transpose to (3,32,32)\n","        elif x.shape == (32, 32, 3):\n","            x = np.transpose(x, (2, 0, 1))\n","\n","        # Convert to a PIL image for torchvision transforms\n","        # x is [3, H, W], so we convert to [H, W, 3] first\n","        x = np.transpose(x, (1, 2, 0))  # shape => (32, 32, 3)\n","        img = Image.fromarray(x.astype(np.uint8))\n","\n","        if self.transform:\n","            img = self.transform(img)\n","        if self.labels is not None:\n","            label = self.labels[idx]\n","            return img, label\n","        else:\n","            return img\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:16:04.182645Z","iopub.status.busy":"2025-03-15T04:16:04.182313Z","iopub.status.idle":"2025-03-15T04:16:06.830702Z","shell.execute_reply":"2025-03-15T04:16:06.830079Z","shell.execute_reply.started":"2025-03-15T04:16:04.182623Z"},"id":"tWnfNgX_zSne","trusted":true},"outputs":[],"source":["# Data transform\n","train_transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),  # Automated augmentation policy\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.4914, 0.4822, 0.4465],\n","                         [0.2023, 0.1994, 0.2010]),\n","\n","    transforms.RandomErasing(p=0.5),\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4942, 0.4851, 0.4504], std=[0.2020, 0.1991, 0.2011]),\n","])\n","\n","train_files = [\n","    '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_1',\n","    '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_2',\n","    '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_3',\n","    '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_4',\n","    '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_5'\n","]\n","\n","test_file = '/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/test_batch'\n","\n","all_data = []\n","all_labels = []\n","for a in train_files:\n","    with open(a, 'rb') as f:\n","        batch = pickle.load(f, encoding='bytes')\n","    all_data.append(batch[b'data'])      # shape => (10000, 3072) each\n","    all_labels.extend(batch[b'labels'])  # 10000 labels\n","\n","train_data = np.vstack(all_data)          # (50000, 3072)\n","train_labels = np.array(all_labels)       # (50000,)\n","\n","# Train dataset\n","train_dataset = CIFARDataset(data=train_data,\n","                             labels=train_labels,\n","                             transform=train_transform)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n","\n","with open(test_file, 'rb') as f:\n","    batch = pickle.load(f, encoding='bytes')\n","test_data = np.array(batch[b'data'])   # (10000, 3072)\n","test_labels = np.array(batch[b'labels'])\n","\n","test_dataset = CIFARDataset(data=test_data,\n","                            labels=test_labels,\n","                            transform=test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model \n","This cell implements a modified ResNet architecture, incorporating Squeeze-and-Excitation (SE) blocks. It defines helper functions (conv1x1), the SEBlock for channel-wise attention, the BasicBlock with dropout and SE, and the full ModifiedResNet model with a custom layer configuration.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:18:09.126286Z","iopub.status.busy":"2025-03-15T04:18:09.125919Z","iopub.status.idle":"2025-03-15T04:18:09.142196Z","shell.execute_reply":"2025-03-15T04:18:09.141148Z","shell.execute_reply.started":"2025-03-15T04:18:09.126256Z"},"id":"qhQAG129zSne","trusted":true},"outputs":[],"source":["def conv1x1(in_channels, out_channels, stride=1, groups=1, bias=False):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, groups=groups, bias=bias)\n","\n","# Convolution-based SE Block (using 1x1 convolutions)\n","class SEBlock(nn.Module):\n","    def __init__(self, channels, reduction=8):\n","        super(SEBlock, self).__init__()\n","        mid_channels = channels // reduction\n","        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.conv1 = conv1x1(in_channels=channels, out_channels=mid_channels, bias=True)\n","        self.activ = nn.ReLU(inplace=True)\n","        self.conv2 = conv1x1(in_channels=mid_channels, out_channels=channels, bias=True)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        w = self.pool(x)\n","        w = self.conv1(w)\n","        w = self.activ(w)\n","        w = self.conv2(w)\n","        w = self.sigmoid(w)\n","        return x * w\n","\n","# BasicBlock with SE \n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1, reduction=8, dropout_rate=0.0):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=False)\n","        self.bn1   = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n","                               padding=1, bias=False)\n","        self.bn2   = nn.BatchNorm2d(planes)\n","\n","        # Use the convolution-based SE Block\n","        self.se    = SEBlock(planes, reduction=reduction)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != planes * self.expansion:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, planes * self.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * self.expansion)\n","            )\n","\n","        self.dropout = nn.Dropout(p=dropout_rate) if dropout_rate > 0.0 else nn.Identity()\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n","        out = self.dropout(out)\n","        out = self.bn2(self.conv2(out))\n","        out = self.se(out)   # Apply SE\n","        out += self.shortcut(x)\n","        out = F.relu(out, inplace=True)\n","        return out\n","\n","# 4. Modified ResNet for CIFAR-10 with [4, 4, 4, 3] residual blocks\n","class ModifiedResNet(nn.Module):\n","    def __init__(self, num_classes=10, reduction=8):\n","        super(ModifiedResNet, self).__init__()\n","        self.in_planes = 32\n","\n","        # Stem: from 3 channels to 32 channels\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1   = nn.BatchNorm2d(32)\n","\n","        self.layer1 = self._make_layer(BasicBlock, 32, num_blocks=4, stride=1,\n","                                       reduction=reduction, dropout_rate=0.0)\n","        self.layer2 = self._make_layer(BasicBlock, 64, num_blocks=4, stride=1,\n","                                       reduction=reduction, dropout_rate=0.2)\n","        self.layer3 = self._make_layer(BasicBlock, 128, num_blocks=4, stride=2,\n","                                       reduction=reduction, dropout_rate=0.3)\n","        self.layer4 = self._make_layer(BasicBlock, 256, num_blocks=3, stride=2,\n","                                       reduction=reduction, dropout_rate=0.3)\n","\n","        # Global average pooling and final classifier\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(256, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride, reduction, dropout_rate):\n","        strides = [stride] + [1] * (num_blocks - 1)\n","        layers = []\n","        for s in strides:\n","            layers.append(block(self.in_planes, planes, s, reduction=reduction, dropout_rate=dropout_rate))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.avgpool(out)\n","        out = torch.flatten(out, 1)\n","        out = self.fc(out)\n","        return out\n","\n","def BuildModifiedResNet():\n","    return ModifiedResNet(num_classes=10, reduction=8)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-03-15T04:36:31.793049Z","iopub.status.busy":"2025-03-15T04:36:31.792712Z","iopub.status.idle":"2025-03-15T04:36:31.921041Z","shell.execute_reply":"2025-03-15T04:36:31.920282Z","shell.execute_reply.started":"2025-03-15T04:36:31.793022Z"},"id":"O7Wk-kYjzSng","outputId":"d30ba6bf-bb65-45a4-93db-d7c912eca0ac","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 32, 32]             864\n","       BatchNorm2d-2           [-1, 32, 32, 32]              64\n","            Conv2d-3           [-1, 32, 32, 32]           9,216\n","       BatchNorm2d-4           [-1, 32, 32, 32]              64\n","          Identity-5           [-1, 32, 32, 32]               0\n","            Conv2d-6           [-1, 32, 32, 32]           9,216\n","       BatchNorm2d-7           [-1, 32, 32, 32]              64\n"," AdaptiveAvgPool2d-8             [-1, 32, 1, 1]               0\n","            Conv2d-9              [-1, 4, 1, 1]             132\n","             ReLU-10              [-1, 4, 1, 1]               0\n","           Conv2d-11             [-1, 32, 1, 1]             160\n","          Sigmoid-12             [-1, 32, 1, 1]               0\n","          SEBlock-13           [-1, 32, 32, 32]               0\n","       BasicBlock-14           [-1, 32, 32, 32]               0\n","           Conv2d-15           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-16           [-1, 32, 32, 32]              64\n","         Identity-17           [-1, 32, 32, 32]               0\n","           Conv2d-18           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-19           [-1, 32, 32, 32]              64\n","AdaptiveAvgPool2d-20             [-1, 32, 1, 1]               0\n","           Conv2d-21              [-1, 4, 1, 1]             132\n","             ReLU-22              [-1, 4, 1, 1]               0\n","           Conv2d-23             [-1, 32, 1, 1]             160\n","          Sigmoid-24             [-1, 32, 1, 1]               0\n","          SEBlock-25           [-1, 32, 32, 32]               0\n","       BasicBlock-26           [-1, 32, 32, 32]               0\n","           Conv2d-27           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-28           [-1, 32, 32, 32]              64\n","         Identity-29           [-1, 32, 32, 32]               0\n","           Conv2d-30           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-31           [-1, 32, 32, 32]              64\n","AdaptiveAvgPool2d-32             [-1, 32, 1, 1]               0\n","           Conv2d-33              [-1, 4, 1, 1]             132\n","             ReLU-34              [-1, 4, 1, 1]               0\n","           Conv2d-35             [-1, 32, 1, 1]             160\n","          Sigmoid-36             [-1, 32, 1, 1]               0\n","          SEBlock-37           [-1, 32, 32, 32]               0\n","       BasicBlock-38           [-1, 32, 32, 32]               0\n","           Conv2d-39           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-40           [-1, 32, 32, 32]              64\n","         Identity-41           [-1, 32, 32, 32]               0\n","           Conv2d-42           [-1, 32, 32, 32]           9,216\n","      BatchNorm2d-43           [-1, 32, 32, 32]              64\n","AdaptiveAvgPool2d-44             [-1, 32, 1, 1]               0\n","           Conv2d-45              [-1, 4, 1, 1]             132\n","             ReLU-46              [-1, 4, 1, 1]               0\n","           Conv2d-47             [-1, 32, 1, 1]             160\n","          Sigmoid-48             [-1, 32, 1, 1]               0\n","          SEBlock-49           [-1, 32, 32, 32]               0\n","       BasicBlock-50           [-1, 32, 32, 32]               0\n","           Conv2d-51           [-1, 64, 32, 32]          18,432\n","      BatchNorm2d-52           [-1, 64, 32, 32]             128\n","          Dropout-53           [-1, 64, 32, 32]               0\n","           Conv2d-54           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-55           [-1, 64, 32, 32]             128\n","AdaptiveAvgPool2d-56             [-1, 64, 1, 1]               0\n","           Conv2d-57              [-1, 8, 1, 1]             520\n","             ReLU-58              [-1, 8, 1, 1]               0\n","           Conv2d-59             [-1, 64, 1, 1]             576\n","          Sigmoid-60             [-1, 64, 1, 1]               0\n","          SEBlock-61           [-1, 64, 32, 32]               0\n","           Conv2d-62           [-1, 64, 32, 32]           2,048\n","      BatchNorm2d-63           [-1, 64, 32, 32]             128\n","       BasicBlock-64           [-1, 64, 32, 32]               0\n","           Conv2d-65           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-66           [-1, 64, 32, 32]             128\n","          Dropout-67           [-1, 64, 32, 32]               0\n","           Conv2d-68           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-69           [-1, 64, 32, 32]             128\n","AdaptiveAvgPool2d-70             [-1, 64, 1, 1]               0\n","           Conv2d-71              [-1, 8, 1, 1]             520\n","             ReLU-72              [-1, 8, 1, 1]               0\n","           Conv2d-73             [-1, 64, 1, 1]             576\n","          Sigmoid-74             [-1, 64, 1, 1]               0\n","          SEBlock-75           [-1, 64, 32, 32]               0\n","       BasicBlock-76           [-1, 64, 32, 32]               0\n","           Conv2d-77           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-78           [-1, 64, 32, 32]             128\n","          Dropout-79           [-1, 64, 32, 32]               0\n","           Conv2d-80           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-81           [-1, 64, 32, 32]             128\n","AdaptiveAvgPool2d-82             [-1, 64, 1, 1]               0\n","           Conv2d-83              [-1, 8, 1, 1]             520\n","             ReLU-84              [-1, 8, 1, 1]               0\n","           Conv2d-85             [-1, 64, 1, 1]             576\n","          Sigmoid-86             [-1, 64, 1, 1]               0\n","          SEBlock-87           [-1, 64, 32, 32]               0\n","       BasicBlock-88           [-1, 64, 32, 32]               0\n","           Conv2d-89           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-90           [-1, 64, 32, 32]             128\n","          Dropout-91           [-1, 64, 32, 32]               0\n","           Conv2d-92           [-1, 64, 32, 32]          36,864\n","      BatchNorm2d-93           [-1, 64, 32, 32]             128\n","AdaptiveAvgPool2d-94             [-1, 64, 1, 1]               0\n","           Conv2d-95              [-1, 8, 1, 1]             520\n","             ReLU-96              [-1, 8, 1, 1]               0\n","           Conv2d-97             [-1, 64, 1, 1]             576\n","          Sigmoid-98             [-1, 64, 1, 1]               0\n","          SEBlock-99           [-1, 64, 32, 32]               0\n","      BasicBlock-100           [-1, 64, 32, 32]               0\n","          Conv2d-101          [-1, 128, 16, 16]          73,728\n","     BatchNorm2d-102          [-1, 128, 16, 16]             256\n","         Dropout-103          [-1, 128, 16, 16]               0\n","          Conv2d-104          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-105          [-1, 128, 16, 16]             256\n","AdaptiveAvgPool2d-106            [-1, 128, 1, 1]               0\n","          Conv2d-107             [-1, 16, 1, 1]           2,064\n","            ReLU-108             [-1, 16, 1, 1]               0\n","          Conv2d-109            [-1, 128, 1, 1]           2,176\n","         Sigmoid-110            [-1, 128, 1, 1]               0\n","         SEBlock-111          [-1, 128, 16, 16]               0\n","          Conv2d-112          [-1, 128, 16, 16]           8,192\n","     BatchNorm2d-113          [-1, 128, 16, 16]             256\n","      BasicBlock-114          [-1, 128, 16, 16]               0\n","          Conv2d-115          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-116          [-1, 128, 16, 16]             256\n","         Dropout-117          [-1, 128, 16, 16]               0\n","          Conv2d-118          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-119          [-1, 128, 16, 16]             256\n","AdaptiveAvgPool2d-120            [-1, 128, 1, 1]               0\n","          Conv2d-121             [-1, 16, 1, 1]           2,064\n","            ReLU-122             [-1, 16, 1, 1]               0\n","          Conv2d-123            [-1, 128, 1, 1]           2,176\n","         Sigmoid-124            [-1, 128, 1, 1]               0\n","         SEBlock-125          [-1, 128, 16, 16]               0\n","      BasicBlock-126          [-1, 128, 16, 16]               0\n","          Conv2d-127          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-128          [-1, 128, 16, 16]             256\n","         Dropout-129          [-1, 128, 16, 16]               0\n","          Conv2d-130          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-131          [-1, 128, 16, 16]             256\n","AdaptiveAvgPool2d-132            [-1, 128, 1, 1]               0\n","          Conv2d-133             [-1, 16, 1, 1]           2,064\n","            ReLU-134             [-1, 16, 1, 1]               0\n","          Conv2d-135            [-1, 128, 1, 1]           2,176\n","         Sigmoid-136            [-1, 128, 1, 1]               0\n","         SEBlock-137          [-1, 128, 16, 16]               0\n","      BasicBlock-138          [-1, 128, 16, 16]               0\n","          Conv2d-139          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-140          [-1, 128, 16, 16]             256\n","         Dropout-141          [-1, 128, 16, 16]               0\n","          Conv2d-142          [-1, 128, 16, 16]         147,456\n","     BatchNorm2d-143          [-1, 128, 16, 16]             256\n","AdaptiveAvgPool2d-144            [-1, 128, 1, 1]               0\n","          Conv2d-145             [-1, 16, 1, 1]           2,064\n","            ReLU-146             [-1, 16, 1, 1]               0\n","          Conv2d-147            [-1, 128, 1, 1]           2,176\n","         Sigmoid-148            [-1, 128, 1, 1]               0\n","         SEBlock-149          [-1, 128, 16, 16]               0\n","      BasicBlock-150          [-1, 128, 16, 16]               0\n","          Conv2d-151            [-1, 256, 8, 8]         294,912\n","     BatchNorm2d-152            [-1, 256, 8, 8]             512\n","         Dropout-153            [-1, 256, 8, 8]               0\n","          Conv2d-154            [-1, 256, 8, 8]         589,824\n","     BatchNorm2d-155            [-1, 256, 8, 8]             512\n","AdaptiveAvgPool2d-156            [-1, 256, 1, 1]               0\n","          Conv2d-157             [-1, 32, 1, 1]           8,224\n","            ReLU-158             [-1, 32, 1, 1]               0\n","          Conv2d-159            [-1, 256, 1, 1]           8,448\n","         Sigmoid-160            [-1, 256, 1, 1]               0\n","         SEBlock-161            [-1, 256, 8, 8]               0\n","          Conv2d-162            [-1, 256, 8, 8]          32,768\n","     BatchNorm2d-163            [-1, 256, 8, 8]             512\n","      BasicBlock-164            [-1, 256, 8, 8]               0\n","          Conv2d-165            [-1, 256, 8, 8]         589,824\n","     BatchNorm2d-166            [-1, 256, 8, 8]             512\n","         Dropout-167            [-1, 256, 8, 8]               0\n","          Conv2d-168            [-1, 256, 8, 8]         589,824\n","     BatchNorm2d-169            [-1, 256, 8, 8]             512\n","AdaptiveAvgPool2d-170            [-1, 256, 1, 1]               0\n","          Conv2d-171             [-1, 32, 1, 1]           8,224\n","            ReLU-172             [-1, 32, 1, 1]               0\n","          Conv2d-173            [-1, 256, 1, 1]           8,448\n","         Sigmoid-174            [-1, 256, 1, 1]               0\n","         SEBlock-175            [-1, 256, 8, 8]               0\n","      BasicBlock-176            [-1, 256, 8, 8]               0\n","          Conv2d-177            [-1, 256, 8, 8]         589,824\n","     BatchNorm2d-178            [-1, 256, 8, 8]             512\n","         Dropout-179            [-1, 256, 8, 8]               0\n","          Conv2d-180            [-1, 256, 8, 8]         589,824\n","     BatchNorm2d-181            [-1, 256, 8, 8]             512\n","AdaptiveAvgPool2d-182            [-1, 256, 1, 1]               0\n","          Conv2d-183             [-1, 32, 1, 1]           8,224\n","            ReLU-184             [-1, 32, 1, 1]               0\n","          Conv2d-185            [-1, 256, 1, 1]           8,448\n","         Sigmoid-186            [-1, 256, 1, 1]               0\n","         SEBlock-187            [-1, 256, 8, 8]               0\n","      BasicBlock-188            [-1, 256, 8, 8]               0\n","AdaptiveAvgPool2d-189            [-1, 256, 1, 1]               0\n","          Linear-190                   [-1, 10]           2,570\n","================================================================\n","Total params: 4,826,746\n","Trainable params: 4,826,746\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 32.92\n","Params size (MB): 18.41\n","Estimated Total Size (MB): 51.34\n","----------------------------------------------------------------\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","model = BuildModifiedResNet().to(device)\n","\n","# Model params\n","summary(model, input_size=(3, 32, 32))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-03-15T04:36:34.656050Z","iopub.status.busy":"2025-03-15T04:36:34.655705Z"},"id":"hojutAJ_zSni","outputId":"b628769f-dce7-431a-87d2-6d33089bf631","trusted":true},"outputs":[],"source":["# Mixed Precision Scaler for AMP\n","scaler = amp.GradScaler()\n","\n","criterion = CrossEntropyLoss(label_smoothing=0.1)\n","base_optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","optimizer = optim2.Lookahead(base_optimizer, k=5, alpha=0.5)\n","\n","num_epochs = 175\n","steps_per_epoch = len(train_loader)\n","\n","scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n","\n","train_losses = []\n","test_losses = []\n","test_accuracies = []\n","\n","# Early stopping \n","best_accuracy = 0.0\n","patience = 10  \n","patience_counter = 0\n","min_delta = 0.01\n","\n","# Mixup augmentation for regularization\n","use_mixup = True\n","\n","def mixup_data(x, y, alpha=0.2):\n","    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n","    index = torch.randperm(x.size(0)).to(x.device)\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    # Mixup loss\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","\n","#  Training \n","for epoch in range(1, num_epochs + 1):\n","    model.train()\n","    running_train_loss = 0.0\n","\n","    for step, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        if use_mixup:\n","            images, y_a, y_b, lam = mixup_data(images, labels, alpha=0.2)\n","        \n","        with autocast(enabled=True):\n","            outputs = model(images)\n","            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam) if use_mixup else criterion(outputs, labels)\n","\n","        scaler.scale(loss).backward()\n","        scaler.unscale_(optimizer)\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # Gradient clipping\n","\n","        scaler.step(optimizer)\n","        scaler.update()\n","        running_train_loss += loss.item()\n","\n","    avg_train_loss = running_train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Evaluation \n","    model.eval()\n","    running_test_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            running_test_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    avg_test_loss = running_test_loss / len(test_loader)\n","    test_losses.append(avg_test_loss)\n","    test_accuracy = 100.0 * correct / total\n","    test_accuracies.append(test_accuracy)\n","\n","    print(f\"Epoch [{epoch}/{num_epochs}] - \"\n","          f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n","          f\"Train Loss: {avg_train_loss:.4f} - \"\n","          f\"Test Loss: {avg_test_loss:.4f} - \"\n","          f\"Test Accuracy: {test_accuracy:.2f}%\")\n","\n","    scheduler.step()\n","\n","    #  Early Stopping\n","    improvement = test_accuracy - best_accuracy\n","    if abs(improvement) > min_delta:\n","        best_accuracy = test_accuracy\n","        patience_counter = 0\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.optimizer.state_dict(),  # Lookahead stores base_optimizer\n","            'train_losses': train_losses,\n","            'test_losses': test_losses,\n","            'test_accuracies': test_accuracies,\n","        }, 'modifiedResnet_cifar10.pth')\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered! No significant improvement for {patience} epochs.\")\n","            break\n","\n","print(\"✅ Model saved as 'modifiedResnet_cifar10.pth'\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"execution":{"iopub.execute_input":"2025-03-15T04:25:24.478354Z","iopub.status.busy":"2025-03-15T04:25:24.478041Z","iopub.status.idle":"2025-03-15T04:25:25.233739Z","shell.execute_reply":"2025-03-15T04:25:25.232609Z","shell.execute_reply.started":"2025-03-15T04:25:24.478328Z"},"id":"5LOwO3eezSni","outputId":"323339ef-0ae2-4eb9-e8ec-5aa46edc93de","trusted":true},"outputs":[],"source":["# Plot Loss Curve\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\", color='blue')\n","plt.plot(range(1, len(test_losses) + 1), test_losses, label=\"Test Loss\", color='orange')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Train and Test Loss')\n","plt.legend()\n","plt.grid()\n","plt.savefig('modifiedResnet_cifar10_loss_curve.png')\n","plt.show()\n","\n","# Plot Accuracy Curve\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy', color='green')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy (%)')\n","plt.title('Test Accuracy Curve')\n","plt.legend()\n","plt.grid()\n","plt.savefig('modifiedResnet_cifar10_accuracy_curve.png')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"L5Xp4mlGzSnj"},"source":["## Test on kaggle test dataset\n","The following code performs inference on the Kaggle test data, and applies Test-Time Augmentation (TTA) by averaging predictions from original and horizontally flipped images to improve generalization."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:25:59.202007Z","iopub.status.busy":"2025-03-15T04:25:59.201640Z","iopub.status.idle":"2025-03-15T04:25:59.470133Z","shell.execute_reply":"2025-03-15T04:25:59.469529Z","shell.execute_reply.started":"2025-03-15T04:25:59.201981Z"},"id":"v-FRwsSvzSnj","trusted":true},"outputs":[],"source":["kaggle_file = '/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl'\n","with open(kaggle_file, 'rb') as f:\n","    content = pickle.load(f, encoding='bytes')\n","kaggle_data = content[b'data']\n","kaggle_data = np.array(kaggle_data)\n","\n","kaggle_test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5121, 0.4964, 0.4612], std=[0.2115, 0.2103, 0.2119]),\n","])\n","\n","\n","\n","kaggle_dataset = CIFARDataset(data=kaggle_data,\n","                              labels=None,  \n","                              transform=kaggle_test_transform)\n","kaggle_loader = DataLoader(kaggle_dataset, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:26:25.351750Z","iopub.status.busy":"2025-03-15T04:26:25.351435Z","iopub.status.idle":"2025-03-15T04:26:34.534307Z","shell.execute_reply":"2025-03-15T04:26:34.533112Z","shell.execute_reply.started":"2025-03-15T04:26:25.351700Z"},"id":"DAWsQ7mdzSnj","trusted":true},"outputs":[],"source":["model.eval()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","all_predictions = []\n","\n","with torch.no_grad():\n","    for images in kaggle_loader:\n","        images = images.to(device)\n","        preds = []\n","        \n","        outputs = model(images)\n","        preds.append(outputs)\n","        # TTA: Horizontal flip predictions\n","        outputs_flip = model(torch.flip(images, dims=[3]))\n","        preds.append(outputs_flip)\n","        \n","        # Average predictions across TTA variants\n","        avg_preds = torch.stack(preds).mean(dim=0)\n","        _, pred_labels = avg_preds.max(1)\n","        all_predictions.extend(pred_labels.cpu().numpy())\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-03-15T04:26:43.151669Z","iopub.status.busy":"2025-03-15T04:26:43.151322Z","iopub.status.idle":"2025-03-15T04:26:43.204231Z","shell.execute_reply":"2025-03-15T04:26:43.203568Z","shell.execute_reply.started":"2025-03-15T04:26:43.151636Z"},"id":"ibO8e9oYzSnj","trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\n","    'ID': list(range(len(all_predictions))),\n","    'Labels': all_predictions\n","})\n","\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":11145869,"sourceId":93057,"sourceType":"competition"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0098257042274f2d92af7b201b6852b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"123de9955d98462aaebe90a6bc4a8483":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24a48554e7a74cffa2146729f04fd6cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e986acb6b6f4c6eb180bb18aad00f37","IPY_MODEL_4c4302e9a3a84ecdb35c58c3ee3d7613","IPY_MODEL_90479ddb6614477991814f01c96185f7"],"layout":"IPY_MODEL_64bba4e71f554236b305bb2b29a920f5"}},"2e986acb6b6f4c6eb180bb18aad00f37":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_123de9955d98462aaebe90a6bc4a8483","placeholder":"​","style":"IPY_MODEL_a244d15525534951a4d1833823538a8f","value":"100%"}},"4c4302e9a3a84ecdb35c58c3ee3d7613":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2ab3570718a42ba93e87b1d30f96935","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6398782fa7d743febb096dbf01708ede","value":50}},"6398782fa7d743febb096dbf01708ede":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64bba4e71f554236b305bb2b29a920f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90479ddb6614477991814f01c96185f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0098257042274f2d92af7b201b6852b5","placeholder":"​","style":"IPY_MODEL_e502a7b2e2454a459939f01469322114","value":" 50/50 [00:07&lt;00:00,  6.85it/s]"}},"a244d15525534951a4d1833823538a8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e502a7b2e2454a459939f01469322114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2ab3570718a42ba93e87b1d30f96935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
